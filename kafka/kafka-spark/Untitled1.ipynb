{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5485dca-2791-4df7-8420-58f057022f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26682d32-241f-4e1e-a057-754e0e5fd537",
   "metadata": {},
   "outputs": [],
   "source": [
    "JAR_PATH = \"/home/khanhpluto/khanhtv/test_spark/zxc/jars_dir/spark-sql-kafka-0-10_2.12-3.2.1.jar,/home/khanhpluto/khanhtv/test_spark/zxc/jars_dir/kafka-clients-3.1.0.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a956356-7c48-4b5f-ba45-7d5ca09bead3",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/10 11:02:15 WARN Utils: Your hostname, khanhpluto-Z10PE-D8-WS resolves to a loopback address: 127.0.1.1; using 192.168.9.45 instead (on interface eno1)\n",
      "23/01/10 11:02:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/01/10 11:02:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/10 11:02:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/01/10 11:02:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark kafka\") \\\n",
    "    .config(\"spark.jars\", JAR_PATH) \\\n",
    "    .config(\"spark.executor.extraClassPath\", JAR_PATH) \\\n",
    "    .config(\"spark.executor.extraLibrary\", JAR_PATH) \\\n",
    "    .config(\"spark.driver.extraClassPath\", JAR_PATH) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae28c2b-bd1c-4615-a67a-d84b18ac189a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3930e1b2-dc15-4d6a-8da0-376103260968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"192.168.9.69:9092\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c98107-e96b-419f-a3bc-2a575a1fb147",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_detail_df1 = df.selectExpr(\"CAST(value AS STRING)\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b94189a-f54c-42f0-bd2a-72c4d453ca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_detail_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bea7aaa3-840d-4a4c-92a5-26ac87f0724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d609a63-a6b0-49f5-a08f-d5cd8097e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_detail_schema = StructType() \\\n",
    "    .add(\"transaction_id\", StringType()) \\\n",
    "    .add(\"transaction_card_type\", StringType()) \\\n",
    "    .add(\"transaction_amount\", StringType()) \\\n",
    "    .add(\"transaction_datetime\", StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f7e75b4-64ad-4028-aa1d-db742040cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_detail_df2 = transaction_detail_df1\\\n",
    "    .select(from_json(col(\"value\"), transaction_detail_schema).alias(\"transaction_detail\"), \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1150370e-1d22-4f23-bb48-aba08e1c5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_detail_df3 = transaction_detail_df2.select(\"transaction_detail.*\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da2268ef-9a0e-4586-ac9c-40518d6c7415",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_detail_df4 = transaction_detail_df3.groupBy(\"transaction_card_type\")\\\n",
    "    .agg({'transaction_amount': 'sum'}).select(\"transaction_card_type\", \\\n",
    "    col(\"sum(transaction_amount)\").alias(\"total_transaction_amount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12a452f8-4f49-4e84-8c93-c6b96cd3be94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Schema of transaction_detail_df4: \n",
      "root\n",
      " |-- transaction_card_type: string (nullable = true)\n",
      " |-- total_transaction_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing Schema of transaction_detail_df4: \")\n",
    "transaction_detail_df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e19e05eb-add3-4d31-b249-651d50c0b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_detail_df5 = transaction_detail_df4.withColumn(\"key\", lit(100))\\\n",
    "                                                .withColumn(\"value\", concat(lit(\"{'transaction_card_type': '\"), \\\n",
    "                                                col(\"transaction_card_type\"), lit(\"', 'total_transaction_amount: '\"), \\\n",
    "                                                col(\"total_transaction_amount\").cast(\"string\"), lit(\"'}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b0217e9-c76c-4c16-9143-a7c93f64bb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Schema of transaction_detail_df5: \n",
      "root\n",
      " |-- transaction_card_type: string (nullable = true)\n",
      " |-- total_transaction_amount: double (nullable = true)\n",
      " |-- key: integer (nullable = false)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing Schema of transaction_detail_df5: \")\n",
    "transaction_detail_df5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "084d4ddf-cfae-4473-b7d3-48fbf93bf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_detail_write_stream = transaction_detail_df5 \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='1 seconds') \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89cfa6b9-d1d4-4aae-80bd-eac53ddf7a51",
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "org/apache/spark/kafka010/KafkaConfigUpdater\n=== Streaming Query ===\nIdentifier: [id = 550ee2e2-b2a9-4300-a5c5-e5ff278db498, runId = d21b36df-177d-4f6c-a0ed-4544b44340ec]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {}\n\nCurrent State: INITIALIZING\nThread State: RUNNABLE",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mStreamingQueryException\u001B[0m                   Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-31-e3ec231af4a6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m \u001B[0mtrans_detail_write_stream\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mawaitTermination\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"PySpark Structured Streaming with Kafka Demo Application Completed.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/khanhtv/test_spark/zxc/venv/lib/python3.6/site-packages/pyspark/sql/streaming.py\u001B[0m in \u001B[0;36mawaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m     99\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mawaitTermination\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m1000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 101\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mawaitTermination\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    102\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    103\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/khanhtv/test_spark/zxc/venv/lib/python3.6/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1321\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1322\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1324\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/khanhtv/test_spark/zxc/venv/lib/python3.6/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mStreamingQueryException\u001B[0m: org/apache/spark/kafka010/KafkaConfigUpdater\n=== Streaming Query ===\nIdentifier: [id = 550ee2e2-b2a9-4300-a5c5-e5ff278db498, runId = d21b36df-177d-4f6c-a0ed-4544b44340ec]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {}\n\nCurrent State: INITIALIZING\nThread State: RUNNABLE"
     ]
    }
   ],
   "source": [
    "trans_detail_write_stream_1 = transaction_detail_df5 \\\n",
    "    .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"192.168.9.69:9092\") \\\n",
    "    .option(\"topic\", \"topic1\") \\\n",
    "    .trigger(processingTime='1 seconds') \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"./py_checkpoint\") \\\n",
    "    .start()\n",
    "\n",
    "trans_detail_write_stream.awaitTermination()\n",
    "\n",
    "print(\"PySpark Structured Streaming with Kafka Demo Application Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "625e884e-fcd7-4cbd-b231-61ca40887353",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data_df=spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(\"data/retails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c7f0cdc-8b45-4100-9a3e-9ab135c634ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/10 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/10 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/10 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/10 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/10 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/10 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/10 8:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/10 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/10 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/10 8:34|     7.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "828d71dc-4994-494d-82bd-45f7aa565e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_detail_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"192.168.9.69:9092\") \\\n",
    "    .option(\"subscribe\", \"topic1\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae41bb4-bbe8-4282-9a2c-3a6d1db67d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
